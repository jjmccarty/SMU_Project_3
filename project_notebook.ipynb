{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mvoz8WR6s-Tc"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import os\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from dotenv import load_dotenv\n",
        "from transformers import pipeline\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import APIChain\n",
        "\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "import sentiment_analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qqjKV6B9AdxI"
      },
      "outputs": [],
      "source": [
        "# Load environment variables.\n",
        "load_dotenv()\n",
        "\n",
        "# Set the model name for our LLMs.\n",
        "GEMINI_MODEL = \"gemini-1.5-flash\"\n",
        "\n",
        "# Store the API key in a variable.\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jScqBgZ-ALrV"
      },
      "outputs": [],
      "source": [
        "# Text description of API spec.\n",
        "spec_str = \"\"\"\n",
        "\n",
        "Instructions for responding:\n",
        "\n",
        "Use the Open Library Search API to get up to 4 books related to the person the book is requested for.  \n",
        "When responding pull any 4 random books from the list do not include any books the users asks to exclude. \n",
        "Provide the response to the user with the title and author of the books in natural language. \n",
        "Ask the user if they want more information about a book\n",
        "\n",
        "When a user asks for information about a book by the title:\n",
        "use Open Library Search API to get the description of the book between 100 and 200 words in length, if there\n",
        "is no description available tell the user a description isn't available and list 4 similar books.\n",
        "\n",
        "When a user asks for information about the author:\n",
        "use Open Library Search API to get information about the author.\n",
        "Respond to the user by telling them about the author and \n",
        "use Open Library Search API to provide 2 more books written by that author.\n",
        "\n",
        "Open Library provides an experimental API to search.\n",
        "\n",
        "WARNING: This API is under active development and may change in future.\n",
        "\n",
        "Overview & Features\n",
        "The Open Library Search API is one of the most convenient and complete ways to retrieve book data on Open Library. The API:\n",
        "\n",
        "Is able to return data for multiple books in a single request/response\n",
        "Returns both Work level information about the book, as well as Edition level information (such as)\n",
        "Author IDs are returned which you can use to fetch the author's image, if available\n",
        "Options are available to return Book Availability along with the response.\n",
        "Powerful sorting options are available, such as star ratings, publication date, and number of editions.\n",
        "Endpoint\n",
        "The endpoint for this API is:\n",
        "https://openlibrary.org/search.json\n",
        "\n",
        "Examples\n",
        "The URL format for API is simple. Take the search URL and add .json to the end. Eg:\n",
        "\n",
        "https://openlibrary.org/search.json?q=the+lord+of+the+rings\n",
        "https://openlibrary.org/search.json?title=the+lord+of+the+rings\n",
        "https://openlibrary.org/search.json?author=tolkien&sort=new\n",
        "https://openlibrary.org/search.json?q=the+lord+of+the+rings&page=2\n",
        "https://openlibrary.org/search/authors.json?q=twain\n",
        "Using Thing IDs to get Images\n",
        "You can use the olid (Open Library ID) for authors and books to fetch covers by olid, e.g.:\n",
        "https://covers.openlibrary.org/a/olid/OL23919A-M.jpg\n",
        "\n",
        "URL Parameters\n",
        "Parameter\tDescription\n",
        "q\tThe solr query. See Search HowTo for sample queries\n",
        "fields\tThe fields to get back from solr. Use the special value * to get all fields (although be prepared for a very large response!).\n",
        "To fetch availability data from archive.org, add the special value, availability. Example: /search.json?q=harry%20potter&fields=*,availability&limit=1. This will fetch the availability data of the first item in the `ia` field.\n",
        "sort\tYou can sort the results by various facets such as new, old, random, or key (which sorts as a string, not as the number stored in the string). For a complete list of sorts facets look here (this link goes to a specific commit, be sure to look at the latest one for changes). The default is to sort by relevance.\n",
        "lang\tThe users language as a two letter (ISO 639-1) language code. This influences but doesn't exclude search results. For example setting this to fr will prefer/display the French edition of a given work, but will still match works that don't have French editions. Adding language:fre on the other hand to the search query will exclude results that don't have a French edition.\n",
        "offset / limit\tUse for pagination.\n",
        "page / limit\tUse for pagination, with limit corresponding to the page size. Note page starts at 1.\n",
        "Include no other text, only the API call URL. Don't use newlines.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "amKx45rg_4ag"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/f6/_37vjycn6436mlkxtc3jkqdw0000gn/T/ipykernel_80699/139866355.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  buffer = ConversationBufferMemory()\n",
            "/var/folders/f6/_37vjycn6436mlkxtc3jkqdw0000gn/T/ipykernel_80699/139866355.py:9: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(llm=moody_llm, verbose=True, memory=buffer)\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model.\n",
        "llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.9)\n",
        "moody_llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.9)\n",
        "\n",
        "# Initialize an object for conversational memory.\n",
        "buffer = ConversationBufferMemory()\n",
        "\n",
        "# Create the chain for conversation, using a ConversationBufferMemory object.\n",
        "conversation = ConversationChain(llm=moody_llm, verbose=True, memory=buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho9zJnADEaCi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "45TORQtx3QIA",
        "outputId": "08476ef3-0472-4335-984b-4ed1536621be"
      },
      "outputs": [],
      "source": [
        "# Create a function that takes in a message.\n",
        "def run_api(msg, history):\n",
        "     # Define a query as a dictionary using the user's input.\n",
        "    query = {\"question\": msg}\n",
        "\n",
        "\n",
        "# Create the API chain from the spec, using the LLM.\n",
        "    chain = APIChain.from_llm_and_api_docs(\n",
        "    llm,\n",
        "    spec_str,\n",
        "    #verbose = True,\n",
        "    limit_to_domains=[\"https://openlibrary.org/\"],\n",
        "    )\n",
        "    try:\n",
        "        # Run the chain using the query, and print the response.\n",
        "        response = chain.invoke(query)\n",
        "        #print(response[\"output\"])\n",
        "        return response[\"output\"]\n",
        "    except:\n",
        "        return \"Sorry, that query generated too large of a result. Please try a different question.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_moody(msg, history):\n",
        "    result = conversation.predict(input=msg)\n",
        "    print(result)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run(msg, history):\n",
        "\n",
        "    sentiment = sentiment_analysis.score(msg)\n",
        "\n",
        "    if sentiment == 'POS':\n",
        "        return run_api(msg, history)\n",
        "    if sentiment == 'NEU':\n",
        "        return run_api(msg, history)\n",
        "    if sentiment == 'NEG':\n",
        "        print(\"Swithing to negative response.\")\n",
        "        return run_moody(msg, history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/dev/lib/python3.10/site-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/dev/lib/python3.10/site-packages/gradio/chat_interface.py:229: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7867\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an instance of the Gradio Interface application function with the following parameters.\n",
        "app = gr.ChatInterface(fn=run, chatbot=gr.Chatbot(value=[(None, \"Welcome 👋 to the Elf Chatbot. Please tell me a little bit about the person you looking to find a book for today?\")],),)\n",
        "\n",
        "# Launch the app\n",
        "app.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
