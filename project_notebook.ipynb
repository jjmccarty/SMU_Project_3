{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mvoz8WR6s-Tc"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "import playsound\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from dotenv import load_dotenv\n",
        "from transformers import pipeline\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import APIChain\n",
        "from gtts import gTTS\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "import sentiment_analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qqjKV6B9AdxI"
      },
      "outputs": [],
      "source": [
        "# Load environment variables.\n",
        "load_dotenv()\n",
        "\n",
        "# Set the model name for our LLMs.\n",
        "GEMINI_MODEL = \"gemini-1.5-flash\"\n",
        "\n",
        "# Store the API key in a variable.\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jScqBgZ-ALrV"
      },
      "outputs": [],
      "source": [
        "# Text description of API spec.\n",
        "spec_str = \"\"\"\n",
        "\n",
        "Instructions for responding:\n",
        "\n",
        "Use the Open Library Search API to get up to 4 books related to the person the book is requested for.  \n",
        "When responding pull any 4 random books from the list do not include any books the users asks to exclude. \n",
        "Provide the response to the user with the title and author of the books in natural language. \n",
        "\n",
        "When a user asks for information about a book by the title:\n",
        "use Open Library Search API to get the description of the book between 100 and 200 words in length, if there\n",
        "is no description available tell the user a description isn't available and list 4 similar books.\n",
        "\n",
        "When a user asks for information about the author:\n",
        "use Open Library Search API to get information about the author.\n",
        "Respond to the user by telling them about the author and \n",
        "use Open Library Search API to provide 2 more books written by that author.\n",
        "\n",
        "Open Library provides an experimental API to search.\n",
        "\n",
        "WARNING: This API is under active development and may change in future.\n",
        "\n",
        "Overview & Features\n",
        "The Open Library Search API is one of the most convenient and complete ways to retrieve book data on Open Library. The API:\n",
        "\n",
        "Is able to return data for multiple books in a single request/response\n",
        "Returns both Work level information about the book, as well as Edition level information (such as)\n",
        "Author IDs are returned which you can use to fetch the author's image, if available\n",
        "Options are available to return Book Availability along with the response.\n",
        "Powerful sorting options are available, such as star ratings, publication date, and number of editions.\n",
        "Endpoint\n",
        "The endpoint for this API is:\n",
        "https://openlibrary.org/search.json\n",
        "\n",
        "Examples\n",
        "The URL format for API is simple. Take the search URL and add .json to the end. Eg:\n",
        "\n",
        "https://openlibrary.org/search.json?q=the+lord+of+the+rings\n",
        "https://openlibrary.org/search.json?title=the+lord+of+the+rings\n",
        "https://openlibrary.org/search.json?author=tolkien&sort=new\n",
        "https://openlibrary.org/search.json?q=the+lord+of+the+rings&page=2\n",
        "https://openlibrary.org/search/authors.json?q=twain\n",
        "Using Thing IDs to get Images\n",
        "You can use the olid (Open Library ID) for authors and books to fetch covers by olid, e.g.:\n",
        "https://covers.openlibrary.org/a/olid/OL23919A-M.jpg\n",
        "\n",
        "URL Parameters\n",
        "Parameter\tDescription\n",
        "q\tThe solr query. See Search HowTo for sample queries\n",
        "fields\tThe fields to get back from solr. Use the special value * to get all fields (although be prepared for a very large response!).\n",
        "To fetch availability data from archive.org, add the special value, availability. Example: /search.json?q=harry%20potter&fields=*,availability&limit=1. This will fetch the availability data of the first item in the `ia` field.\n",
        "sort\tYou can sort the results by various facets such as new, old, random, or key (which sorts as a string, not as the number stored in the string). For a complete list of sorts facets look here (this link goes to a specific commit, be sure to look at the latest one for changes). The default is to sort by relevance.\n",
        "lang\tThe users language as a two letter (ISO 639-1) language code. This influences but doesn't exclude search results. For example setting this to fr will prefer/display the French edition of a given work, but will still match works that don't have French editions. Adding language:fre on the other hand to the search query will exclude results that don't have a French edition.\n",
        "offset / limit\tUse for pagination.\n",
        "page / limit\tUse for pagination, with limit corresponding to the page size. Note page starts at 1.\n",
        "Include no other text, only the API call URL. Don't use newlines.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "amKx45rg_4ag"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/f6/_37vjycn6436mlkxtc3jkqdw0000gn/T/ipykernel_5258/139866355.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  buffer = ConversationBufferMemory()\n",
            "/var/folders/f6/_37vjycn6436mlkxtc3jkqdw0000gn/T/ipykernel_5258/139866355.py:9: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(llm=moody_llm, verbose=True, memory=buffer)\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model.\n",
        "llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.9)\n",
        "moody_llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.9)\n",
        "\n",
        "# Initialize an object for conversational memory.\n",
        "buffer = ConversationBufferMemory()\n",
        "\n",
        "# Create the chain for conversation, using a ConversationBufferMemory object.\n",
        "conversation = ConversationChain(llm=moody_llm, verbose=True, memory=buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho9zJnADEaCi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "45TORQtx3QIA",
        "outputId": "08476ef3-0472-4335-984b-4ed1536621be"
      },
      "outputs": [],
      "source": [
        "# Create a function that takes in a message and executes the openlibrary.org API\n",
        "def run_api(msg, history):\n",
        "     # Define a query as a dictionary using the user's input.\n",
        "    query = {\"question\": msg}\n",
        "\n",
        "\n",
        "# Create the API chain from the spec, using the LLM.\n",
        "    chain = APIChain.from_llm_and_api_docs(\n",
        "    llm,\n",
        "    spec_str,\n",
        "    #verbose = True,\n",
        "    limit_to_domains=[\"https://openlibrary.org/\"],\n",
        "    )\n",
        "    try:\n",
        "        # Run the chain using the query, and print the response.\n",
        "        response = chain.invoke(query)\n",
        "        #print(response[\"output\"])\n",
        "        return response[\"output\"]\n",
        "    except:\n",
        "        return \"Sorry, that query generated too large of a result. Please try a different question.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function is run if the user enters a very negative message.\n",
        "def run_moody(msg, history):\n",
        "    result = conversation.predict(input=msg)\n",
        "    #print(result)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flag is set if the negative sentiment is activated, keeps user in that LLM\n",
        "neg_flag = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_to_speech(message):\n",
        "    try:\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
        "        # Convert the text response to speech.\n",
        "        tts = gTTS(message, lang='en')        \n",
        "        tts.save(temp_file.name)\n",
        "        return temp_file.name\n",
        "    except Exception as e:\n",
        "        error_message = f\"Sorry, an error occurred: {e}. Please try again.\"\n",
        "        tts = gTTS(error_message, lang='en')\n",
        "        tts.save(temp_file.name)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function run by the Gradio chatbot.  This will perform sentiment analysis\n",
        "# and call the correct LLM based on if the sentiment is negative \n",
        "def run(msg, history):\n",
        "\n",
        "    sentiment = sentiment_analysis.score(msg)\n",
        "\n",
        "    returned_message = \"\"\n",
        "\n",
        "    #Initialize history if it's None\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    #Append user message to the history\n",
        "    #Append user message to the history imnediately\n",
        "    history.append((returned_message, \"\"))\n",
        "    yield history, None, history # Update the chat with user input\n",
        "\n",
        "\n",
        "    # obtain the llm response\n",
        "    # if the sentiment is negative, use the straight LLM, otherwise \n",
        "    # will utilize the openlibrary.org API\n",
        "    \n",
        "    # if we ever tripped the negativity flag, stay with that llm\n",
        "    if neg_flag == True:\n",
        "        sentiment == 'NEG'\n",
        "\n",
        "    if sentiment == 'POS':\n",
        "        returned_message =  run_api(msg, history)\n",
        "    if sentiment == 'NEU':\n",
        "        returned_message =  run_api(msg, history)\n",
        "    if sentiment == 'NEG':\n",
        "        print(\"Swithing to negative response.\")\n",
        "        returned_message = run_moody(msg, history)\n",
        "    \n",
        "    bot_message = returned_message\n",
        "\n",
        "    #updaste the last message in history with the bot response\n",
        "    history[-1] = (msg, bot_message)    \n",
        "\n",
        "    # builds the text to speech component\n",
        "    audio_file = text_to_speech(returned_message)\n",
        "\n",
        "    yield history, audio_file, history\n",
        "\n",
        "    return returned_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/dev/lib/python3.10/site-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=run,\n",
        "    inputs=[\n",
        "        # gr.Textbox(label=\"Enter text\", placeholder=\"Ask about books...\"),\n",
        "         gr.Textbox(label=\"Enter your message\", placeholder=\"Type your question here...\"),\n",
        "        gr.State(value=[])  # To keep track of the chat history\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Chatbot(label=\"Chat History\"),\n",
        "        gr.Audio(label=\"Audio Response\"),\n",
        "        gr.State(value=[])\n",
        "    ],\n",
        "    title=\"Book Search Chatbot\",\n",
        "    description=\"A chatbot that searches books using Open Library API.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
